# -*- coding: utf-8 -*-
"""Interview preparation system.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OjiI8-zAaDTyfi3kLA1J6yIIjPL-CGrt
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install langchain-community --quiet

# Commented out IPython magic to ensure Python compatibility.
# %pip install pypdf --quiet

# Commented out IPython magic to ensure Python compatibility.
# %pip install -U langchain langchain-community

# Commented out IPython magic to ensure Python compatibility.
# %pip install matplotlib-venn

from langchain_community.document_loaders import PyPDFLoader

loader=PyPDFLoader("/content/Machine_Learning.pdf")
pdf_pages=loader.load()

pdf_pages[0]

# Commented out IPython magic to ensure Python compatibility.
# %pip install langchain-text-splitters

from langchain_text_splitters import RecursiveCharacterTextSplitter

chunk_size = 1024
chunk_overlap = 200

splitter = RecursiveCharacterTextSplitter(
    chunk_size=chunk_size,
    chunk_overlap=chunk_overlap
)

chunked_docs = splitter.split_documents(pdf_pages)

len(pdf_pages)

len(chunked_docs)

"""**EMBEDDING**"""

from langchain_community.embeddings import HuggingFaceEmbeddings

multilingual_embeddings = HuggingFaceEmbeddings(model_name='intfloat/multilingual-e5-large')

all_mini_embeddings = HuggingFaceEmbeddings(
    model_name="all-MiniLM-L6-v2"
)

multilingual_embeddings = HuggingFaceEmbeddings(model_name='intfloat/multilingual-e5-large')

sen1="I Love ice creams"
sen2="I like to have ice creams"
sen3="Machine Learning"
sen4="I Love ice creams"

len(sen1)

len(all_mini_embeddings.embed_query(sen1))

len(multilingual_embeddings.embed_query(sen1))

all_mini_embeddings.embed_query(sen1)

emd1=(all_mini_embeddings.embed_query(sen1))
emd2=(all_mini_embeddings.embed_query(sen2))
emd3=(all_mini_embeddings.embed_query(sen3))
emd4=(all_mini_embeddings.embed_query(sen4))

embedding1=(multilingual_embeddings.embed_query(sen1))
embedding2=(multilingual_embeddings.embed_query(sen3))

import numpy as np

np.dot(emd1,emd3)

"""Similarity Score	Meaning
0.80 – 1.00	Very similar

0.60 – 0.79	Related

0.40 – 0.59	Somewhat related

< 0.40	Not similar
"""

np.dot(emd1,emd4)

"""When to use which model Model	Use Case           
all-MiniLM-L6-v2	   English-only RAG, fast

Multilingual MiniLM	 Tamil, Hindi, mixed   languages
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install chromadb --quiet

from langchain_community.vectorstores import Chroma

"""Leveraging ChromaDB for Vector Embeddings - A Comprehensive ...ChromaDB (or Chroma) is an open-source, developer-friendly vector database designed for building AI applications, especially those using Large Language Models (LLMs)."""

persist_directory = "/db/chroma/"

vectordb = Chroma.from_documents(
    documents=chunked_docs,
    embedding=multilingual_embeddings,
    persist_directory=persist_directory
)

question = "Explain the Random forest method in machine learning?"

vectordb.similarity_search(question, k=3)

vectordb.similarity_search_with_score(question, k=3)

"""Retrieval"""

vectordb.max_marginal_relevance_search(question, k=3, fetch_k=10)

vectordb.similarity_search(
    question,
    k=3,
    filter={"source":"/content/Machine_Learning.pdf"}
)

# Commented out IPython magic to ensure Python compatibility.
# %pip install langchain-groq --quiet

import getpass
import os

if "GROQ_API_KEY" not in os.environ:
    print("Before getpass")
    os.environ["GROQ_API_KEY"] = getpass.getpass("Enter your Groq API key: ")
    print("After getpass")

from langchain_groq import ChatGroq

llm = ChatGroq(
    model="llama-3.1-8b-instant",
    temperature=0,
    max_tokens=250,
)

import os
os.environ["GROQ_API_KEY"] = "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"

!pip install -U langchain langchain-groq groq

import os
print(os.getenv("GROQ_API_KEY"))

from langchain_groq import ChatGroq

llm = ChatGroq(
    model="llama-3.1-8b-instant",
    temperature=0.7
)

response = llm.invoke("Write a essay about machine learning")
print(response.content)

response.content

system_prompt = (
        "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question."
        "If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise."
        "Answer all questions to the best of your ability."
)

from langchain_core.messages import SystemMessage, HumanMessage, AIMessage

system_message = [SystemMessage(content = system_prompt)]

question = "Explain the Random forest method in machine learning?"

vectordb.similarity_search(question, k=3)

docs = vectordb.similarity_search_with_score(question, k=5)

import pandas as pd

_docs = pd.DataFrame(
    [(question, doc[0].page_content, doc[0].metadata.get('source'), doc[0].metadata.get('page'), doc[1]) for doc in docs],
    columns=['query', 'paragraph', 'document', 'page_number', 'relevant_score']
)

_docs

context = "\n\n".join(_docs['paragraph'])

context

human_message = [HumanMessage(content = context + question)]

result = llm.invoke(system_message + human_message)

result.content

question = "what are the models in meachine learning"

docs = vectordb.similarity_search_with_score(question, k=5)

_docs = pd.DataFrame(
    [(question, doc[0].page_content, doc[0].metadata.get('source'), doc[0].metadata.get('page'), doc[1]) for doc in docs],
    columns=['query', 'paragraph', 'document', 'page_number', 'relevant_score']
)

context = "\n\n".join(_docs['paragraph'])

human_message = [HumanMessage(content = context + question)]
response = llm.invoke(system_message + human_message)

response.content

context

# Commented out IPython magic to ensure Python compatibility.
# %pip install langgraph --quiet

